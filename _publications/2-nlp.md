---
title: "Benchmarking Large Language Models for Persian: A Preliminary Study Focusing on ChatGPT"
accepted: "LREC-COLING 2024, Apr 2024"
paperurl: "https://arxiv.org/abs/2404.02403"
codeurl: "https://github.com/Nesabbasi/Benchmarking_ChatGPT_for_Persian"
authors: "Amirhossein Abaskohi, Sara Baruni, Mostafa Masoudi, and Mohammad Hadi Babalou, Nesa Abbasimoghadam, Ali Edalat, Sepehr Kamahi, Samin Mahdizadeh Sani, Nikoo Naghavian, Danial Namazifard, Pouya Sadeghi, Yadollah Yaghoobzadeh"
abstract: "This project investigates the effectiveness of large language models (LLMs) for the Persian language. While models like ChatGPT demonstrate exceptional performance in English, their effectiveness in low-resource languages like Persian remains underexplored. We conducted a comprehensive benchmarking study focusing on GPT-3.5-turbo, GPT-4, and OpenChat-3.5 across various tasks, including classic, reasoning, and knowledge-based categories. Notably, we introduced two new benchmarks for reasoning tasks due to the scarcity of existing datasets. Our findings indicate that while LLMs excel in reasoning, task-specific fine-tuned models often outperform them in specific areas, underscoring the potential for enhancing LLM performance in Persian."
---